<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Eric's Notes</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/stylesheets/main.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header>
    <h1 class="center"><a href="/">Weise-ipedia</a></h1>
    <hr>
</header>
<h1 id="basic-probability">Basic Probability</h1>
<p>Some fundamental probability axioms and results used in Information Theory.</p>
<h2 id="random-variables">Random Variables</h2>
<p>This page is only going to deal with <strong>discrete</strong> random variables.</p>
<ul>
<li>An <strong>experiment</strong> or a <strong>random variable</strong> is an event with random outcomes.</li>
<li>The <strong>set of possible outcomes</strong> of an experiment are described by capital letters (<span class="math inline">\(X, Y, Z\)</span>).</li>
<li>A single <strong>outcome</strong> of an experiment is described by a lower case letter (<span class="math inline">\(x,y,z\)</span>) and is said to exist in the set of possible outcomes (<span class="math inline">\(x \in X\)</span> and <span class="math inline">\(y \in Y\)</span>).</li>
<li>The <strong>probability mass function</strong>, <span class="math inline">\(p(x)\)</span>, tells us the likelyhood of the outcome <span class="math inline">\(x\)</span> occurring when experiment <span class="math inline">\(X\)</span> is performed. (If we were dealing with continuous random variables we would call <span class="math inline">\(p(x)\)</span> a probability distribution function, PDF)</li>
</ul>
<h4 id="example">Example</h4>
<p>A coin flip is an experiment with two outcomes, heads or tails, or <span class="math inline">\(X = \{heads, tails\}\)</span>. If the coin is fair (if one side isn’t weighted) then the probability mass function is:</p>
<p><span class="math display">\[ p(x) = \begin{cases}
    \frac{1}{2} \text{ $x$ is heads} \\
    \frac{1}{2} \text{ $x$ is tails}
\end{cases} \]</span></p>
<h2 id="joint-probability-mass-functions">Joint Probability Mass Functions</h2>
<p>If There are two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, which have outcomes <span class="math inline">\(\{x_1, \ldots, x_n\}\)</span> and <span class="math inline">\(\{y_1, \ldots, y_m\}\)</span>, respectively, then the vector <span class="math inline">\((X,Y)\)</span> is also a discrete random variable. It has the probility mass function <span class="math inline">\(p(x,y)\)</span>, also called the <strong>joint pmf</strong>. This PMF tells us that there is a <span class="math inline">\(p(x_i,y_j)\)</span> probability of observing <span class="math inline">\(X=x_i\)</span> and <span class="math inline">\(Y=y_j\)</span> together.</p>
<p>Sometimes we write the joint probability mass function as <span class="math inline">\(p(X \cap Y)\)</span>. This is just a restatement that the set of possible outcomes <span class="math inline">\((X,Y)\)</span> is the intersection of the outcomes of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. You will see <span class="math inline">\(p(X,Y)\)</span> and <span class="math inline">\(P(X \cap Y)\)</span> used interchangeably.</p>
<p>A joint probability mass function must satisfy:</p>
<ol type="1">
<li><span class="math inline">\(0 \leq p(x,y) \leq 1\)</span>, and</li>
<li><span class="math inline">\(\sum_{i=1}^n \sum_{j=1}^m p(x_i,y_j) = 1\)</span></li>
</ol>
<p>This logic can be extended to any number of random variables. So, if we have a set of <span class="math inline">\(n\)</span> random events, the probability mass function will be <span class="math inline">\(p(x_1, \ldots, x_n)\)</span></p>
<h4 id="example-1">Example</h4>
<p>Consider flipping a fair coin (call this <span class="math inline">\(X\)</span>) and rolling a fair dice (<span class="math inline">\(Y\)</span>). Then the joint probability function <span class="math inline">\(p(x,y)\)</span> for <span class="math inline">\(x \in X\)</span> and <span class="math inline">\(y \in Y\)</span>:</p>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span>  <span class="math inline">\(y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>heads</td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
</tr>
<tr class="even">
<td>tails</td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
<td><span class="math inline">\(\frac{1}{12}\)</span></td>
</tr>
</tbody>
</table>
<h2 id="expected-value">Expected Value</h2>
<p>The expected value of a function <span class="math inline">\(g\)</span> acting on a discrete random variable <span class="math inline">\(X\)</span> is defined as:</p>
<p><span class="math display">\[ E[g(X)] \equiv E_p g(X) = \sum_{x \in X} p(x)g(x) \]</span></p>
<h2 id="conditional-probability">Conditional Probability</h2>
<p>Consider two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, where the outcome of <span class="math inline">\(Y\)</span> is dependent on the outcome of <span class="math inline">\(X\)</span>. The following relationship is can be derived by multiplying the probabilities of two disjoint events <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_j|x_i\)</span>: <span class="math display">\[ p(y_j|x_i) = \frac{p(y_j \cap x_i)}{p(x_i)} \]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent then <span class="math inline">\(p(y_j \cap x_i) = p(y_j)p(x_i)\)</span> and this reduces to <span class="math inline">\(p(y_j|x_i) = p(y_j)\)</span>, as expected.</p>
<h4 id="example-2">Example</h4>
<p>Consider flipping a coin (<span class="math inline">\(X\)</span>) then drawing a ball out of a bag (<span class="math inline">\(Y\)</span>), but the bag changes depending on the result of the coin flip. If the coin is heads the bag has two white and one red balls, but if the bag is tails the bag has three white and two red balls. Then, <span class="math display">\[
p(white|heads) = \frac{2}{3}, \ \
p(red|heads) = \frac{1}{3} \\
p(white|tails) = \frac{3}{5}, \ \
p(red|tails) = \frac{2}{5} \]</span></p>
<p>Using <span class="math inline">\(p(x,y) = p(y|x)p(x)\)</span> we can calculate the joint probability mass function:</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span>  <span class="math inline">\(y\)</span></th>
<th>white</th>
<th>red</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>heads</td>
<td><span class="math inline">\(\frac{1}{3}\)</span></td>
<td><span class="math inline">\(\frac{1}{6}\)</span></td>
</tr>
<tr class="even">
<td>tails</td>
<td><span class="math inline">\(\frac{3}{10}\)</span></td>
<td><span class="math inline">\(\frac{1}{5}\)</span></td>
</tr>
</tbody>
</table>
<p>We can verify these probabilities by counting all the possibe outcomes.</p>
<h2 id="references">References</h2>
<ol type="1">
<li><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading7a.pdf">Introduction to Probability and Statistics, MIT OpenCourseWare</a></li>
</ol>
<div class="center">
    <hr style="margin-top: 5ch;">
    <p><b>Notes on Computers</b></p>
    <p>
             <a href="/aws">AWS</a>
    &middot; <a href="/bash">Bash</a>
    &middot; <a href="/c-lang">C</a>
    &middot; <a href="/cpp-lang">C++</a>
    &middot; <a href="/cybersecurity">Cyber Security</a>
    &middot; <a href="/git">Git</a>
    &middot; <a href="/latex">LaTeX</a>
    &middot; <a href="/linux">Linux</a>
    &middot; <a href="/networking">Networking</a>
    &middot; <a href="/python">Python</a>
    &middot; <a href="/raspberry-pi">Raspberry Pi</a>
    &middot; <a href="/tools">Tools</a>
    &middot; <a href="/vim">Vim</a>
    </p>
    <p><b>Notes on Math &amp; Physics</b></p>
    <p>
             <a href="/information-theory">Information Theory</a>
    &middot; <a href="/linear-algebra">Linear Algebra</a>
    &middot; <a href="/solid-state-physics">Solid State Physics</a>
    </p>
    <hr>
    <p>Copyright 2021 &middot; <a href="https://ericdweise.com">Eric D. Weise</a></p>
</div>
</body>
</html>
